---
title: Introduction
description: 'Deploy AI agents to production in seconds, not hours'
icon: 'book-open'
---

import NeedHelp from '/snippets/need-help.mdx';

RunAgent lets you deploy Python-based AI agents and access them from any language with full streaming support.

## Key Benefits

- **Deploy agents in seconds** - Serverless infrastructure with automatic scaling
- **Access from Python, JS, Go, Rust and coming more** - Native-feeling SDKs for every language
- **Built-in streaming** - Real-time token streaming for responsive applications

## Quick Actions

<CardGroup cols={3}>
  <Card title="Deploy your first agent" icon="rocket" href="/tutorials/deploy-your-first-agent">
    Get started with a complete tutorial
  </Card>
  <Card title="Use frameworks" icon="code" href="/how-to/frameworks/langgraph">
    Integrate with popular AI frameworks
  </Card>
  <Card title="Explore SDKs" icon="terminal" href="/reference/sdk/overview">
    Access agents from any language
  </Card>
</CardGroup>

<img
  className="block dark:hidden"
  src="/images/runagent_update.svg"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/images/runagent_update.svg"
  alt="Hero Dark"
/>

## The Problem Every AI Developer Faces

Suppose you've built an incredible AI agent in Python. It uses **LangGraph for complex reasoning**, leverages **powerful tools**, and produces **amazing results**. Your team loves it!

### Then reality hits:

Your whole team is excited to use it! But the **frontend team** needs to access it in JavaScript, your **mobile app team** wants it in Kotlin, your **Unity team** wants it in C#, your **systems team** requires it in Rust.

### The traditional approach?
Build separate implementations, REST APIs, WebSocket handlers...

<Danger>**Sound exhausting?** That's because it is!</Danger>

## What RunAgent Actually Does

RunAgent fundamentally changes how AI agent deployment works. Your Python function signatures automatically become API contracts (REST or Streaming) for every supported language.

<Tip>Once `runagent.config.json` of your project points to your Python function, they are automatically converted to corresponding API endpoints, and all language SDKs automatically adapt. No API versioning, no breaking changes.</Tip>

The fastest way to experience the magic is with **RunAgent CLI**:

<Steps>

  <Step title="Install the RunAgent CLI">
  This installs the powerful RunAgent CLI, which is used to deploy and manage your agents.
   ```bash
   pip install runagent
   ```
  </Step>

  <Step title="Initialize Your Agent">

  Let's start with a minimal Agent example.
  ```bash
  runagent init my_agent
  cd my_agent
  ```
  
  This creates a new directory with the basic structure you need to get started.
  </Step>

  <Step title="Define Your Agent Function">
  
  In your agent codebase (for example, `main.py`), define a function that will serve as your agent's entrypoint:
  
  ```python main.py
def mock_response(message, role="user"):
    """Test the mock agent with non-streaming responses"""
    # Your agent logic here
    # Process the message and generate a response
    return response.content
  ```
  
  This `mock_response` function is one of the invocation functions for our agent. It takes a message and role as parameters and returns the agent's response.
  </Step>

  <Step title="Configure the Agent Entrypoint">
  
  Open the `runagent.config.json` file and add your function as an entrypoint:
  
  ```json
"entrypoints": [
    {
      "file": "main.py",
      "module": "mock_response",
      "tag": "minimal"
    }
]
  ```
  
  The `file` specifies where your function lives, `module` is the function name, and `tag` is a label you'll use to call this specific entrypoint.
  </Step>

  <Step title="Run the Agent Locally">
  
  Start your agent locally to test it:
  
  ```bash
runagent serve .
```

You will see output similar to:

```bash
Agent Details:
- Agent ID: f7066c98-0eb2-488c-bb37-a869a93d51ce
- Host: 127.0.0.1
- Port: 8451
- Framework: default
- Status: ready

Server running at: http://127.0.0.1:8451
API Documentation: http://127.0.0.1:8451/docs

Available endpoints:
- POST /api/v1/agents/.../execute/minimal - Run your agent

INFO: Uvicorn running on http://127.0.0.1:8451 (Press CTRL+C to quit)
  ```
  
  That's it! Your agent is now running and accessible through standard REST API as well as all RunAgent SDKs.
  </Step>

<Step title="Use the Agent in Your Application">

Using the RunAgent SDKs, you can access your agent from any supported language. You only need the agent ID and the entrypoint tag. Your `mock_response` function now becomes accessible in multiple languages:

<CodeGroup>

```python Python 
from runagent import RunAgentClient

# Create a client pointing to your agent
ra = RunAgentClient(
    agent_id="f7066c98-0eb2-488c-bb37-a869a93d51ce",
    entrypoint_tag="minimal",
    local=True
)

# Call your agent
agent_result = ra.run(
    role="user",
    message="Analyze the benefits of remote work for software teams"
)

print(agent_result)
```

```javascript JavaScript
import { RunAgentClient } from 'runagent';

// Create a client pointing to your agent
const ra = new RunAgentClient({
  agentId: 'f7066c98-0eb2-488c-bb37-a869a93d51ce',
  entrypointTag: 'minimal',
  local: true,
});

// Call your agent
const analysis = await ra.run({
    role: "user",
    message: "Analyze the benefits of remote work for software teams"
});

console.log(analysis);
```

```rust Rust  
use runagent::client::RunAgentClient;
use serde_json::json;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create a client pointing to your agent
    let client = RunAgentClient::new(
        "f7066c98-0eb2-488c-bb37-a869a93d51ce", 
        "minimal", 
        true
    ).await?;
    
    // Call your agent
    let response = client.run(&[
        ("role", json!("user")),
        ("message", json!("Analyze the benefits of remote work for software teams"))
    ]).await?;
    
    println!("Response: {}", response);
    Ok(())
}
```

```go Go
package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/runagent-dev/runagent-go/pkg/client"
)

func main() {
	// Create a client pointing to your agent
	agentClient, err := client.NewWithAddress(
		"f7066c98-0eb2-488c-bb37-a869a93d51ce",
		"minimal",
		true,
		"localhost",
		8453,
	)
	if err != nil {
		log.Fatalf("Failed to create client: %v", err)
	}
	defer agentClient.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 2*time.Minute)
	defer cancel()

	// Call your agent
	result, err := agentClient.Run(ctx, map[string]interface{}{
		"message": "Analyze the benefits of remote work for software teams",
		"role": "user",
	})
	if err != nil {
		log.Fatalf("Failed to run agent: %v", err)
	}

	fmt.Printf("Result: %v\n", result)
}
```
</CodeGroup>

Notice how the same agent logic is accessible from all languages with idiomatic syntax for each.

</Step>
</Steps>

That's the core workflow. You're now ready to **deploy your AI agents** and **integrate them into your applications**.

## From Local Testing to Production in Seconds

Once you've tested your agent locally and confirmed everything works, deploying to production is just one command away.

**RunAgent Cloud** is the fastest AI agent deployment platform, designed to take your local agent from development to production instantly. No complex infrastructure setup, no deployment headaches.

### Deploy to Production

```bash
runagent deploy .
```

That's it. Your agent is now live and accessible globally with:

- **Automatic scaling** - Handle one request or one million
- **Global edge network** - Low latency worldwide
- **Zero infrastructure management** - Focus on your agent, not servers
- **Production-ready security** - Enterprise-grade isolation and security

Your deployed agent gets a production URL and agent ID. Simply update your SDK clients to use the production endpoint:

```python
# Switch from local to production
ra = RunAgentClient(
    agent_id="your-production-agent-id",
    entrypoint_tag="minimal",
    local=False  # Now pointing to RunAgent Cloud
)
```

The same code you tested locally now runs in a fully managed production environment.

<Card 
  title="Learn More About Deployment" 
  icon="cloud-arrow-up" 
  href="/deployment/cloud-deployment"
>
  Explore advanced deployment options, environment variables, and monitoring capabilities.
</Card>



## Agent Framework Support

RunAgent works with any Python-based AI agent framework:

<CardGroup cols={3}>
  <Card
    title="LangGraph"
    href="/how-to/frameworks/langgraph"
  >
    Deploy your LangGraph agents with built-in state management and workflow execution.
  </Card>
  <Card 
    title="CrewAI" 
    href="/how-to/frameworks/crewai"
  >
    Deploy multi-agent CrewAI systems with coordinated execution and monitoring.
  </Card>
  <Card 
    title="AutoGen" 
    href="/how-to/frameworks/autogen"
  >
    Deploy Microsoft AutoGen multi-agent conversations with step-by-step execution.
  </Card>
  <Card 
    title="AG2" 
    href="/how-to/frameworks/ag2"
  >
    Deploy AG2 conversational agents with fact-checking and multi-turn interactions.
  </Card>
  <Card 
    title="Agno" 
    href="/how-to/frameworks/agno"
  >
    Deploy Agno AI agents with print response capabilities and streaming support.
  </Card>
  <Card 
    title="Letta" 
    href="/how-to/frameworks/letta"
  >
    Deploy Letta memory-enabled agents with persistent conversations and tool integration.
  </Card>
</CardGroup>
  <Card 
    title="Custom Framework" 
    href="/how-to/frameworks/custom"
    horizontal
  >
    Use any Python-based framework by defining simple entrypoint functions.
  </Card>

## Multi-Language SDK Support

RunAgent provides native-like access to your deployed agents across multiple languages:

<CardGroup cols={2}>
  <Card
    title="Python SDK"
    icon="python"
    href="/reference/sdk/python"
  >
    Python client with streaming capabilities. Access your agents like local functions with full type safety and Python idioms.
  </Card>
  <Card 
    title="JavaScript SDK" 
    icon="js" 
    href="/reference/sdk/javascript"
  >
    Full TypeScript support with streaming and Promise-based APIs. Perfect for modern web applications with async/await patterns.
  </Card>
  <Card 
    title="Rust SDK" 
    icon="rust" 
    href="/reference/sdk/rust"
  >
    High-performance async SDK with futures and streaming support. Zero-cost abstractions for systems programming and performance-critical applications.
  </Card>
  <Card 
    title="Go SDK" 
    icon="golang" 
    href="/reference/sdk/go"
  >
    Idiomatic Go client with context-aware operations and channel-based streaming. Built for concurrent, scalable applications.
  </Card>
</CardGroup>

<Card 
  title="All Language SDKs" 
  icon="code" 
  href="/reference/sdk/overview"
  horizontal
>
  We're actively developing SDKs for additional languages including C++, C#, Java, and PHP. Want to contribute or request a specific language? Join our Discord community.
</Card>

## Real-Time Streaming Across Languages

In addition to standard REST API responses, you can also stream your agent responses seamlessly through our SDKs. 

When your entrypoint streams a response, RunAgent makes it feel native in every language SDK:

<Steps>

  <Step title="Define a Streaming Function">

  Create a function in your agent codebase that returns an Iterator (in this case in `main.py`):

  ```python main.py
def mock_response_stream(message, role="user") -> Iterator[str]:
    """Test the mock agent with streaming responses"""
    # Your streaming agent logic here
    # Yield chunks of the response as they're generated
    for chunk in generate_response_chunks(message):
        yield chunk
  ```
  
  This `mock_response_stream` function will return an Iterator that yields response chunks as they're generated.
  </Step>

  <Step title="Configure the Streaming Entrypoint">
  
  Add this function to your `runagent.config.json` file as another entrypoint:
  
  ```json
"entrypoints": [
    {
      "file": "main.py",
      "module": "mock_response_stream",
      "tag": "minimal_stream"
    }
]
  ```
  
  <Note>The tag for a streaming entrypoint must end with a `_stream` suffix. This is how RunAgent identifies it as a streaming endpoint.</Note>
  </Step>

  <Step title="Run the Agent Locally">
  
  Spin up the agent just like before. Now you have an additional streaming entrypoint available.
  
  ```bash
runagent serve .
```

  </Step>

<Step title="Use Streaming in Your Application">

Using the RunAgent SDKs, you can stream responses from your agent. The streaming experience feels natural in each language:

<CodeGroup>

```python Python 
from runagent import RunAgentClient

# Create a client with the streaming tag
ra = RunAgentClient(
    agent_id="f7066c98-0eb2-488c-bb37-a869a93d51ce",
    entrypoint_tag="minimal_stream",
    local=True
)

# Stream the response chunk by chunk
for chunk in ra.run(
    role="user",
    message="Analyze the benefits of remote work for software teams"
):
    print(chunk, end='', flush=True)
```

```javascript JavaScript
import { RunAgentClient } from 'runagent';

// Create a client with the streaming tag
const client = new RunAgentClient({
    agentId: 'f7066c98-0eb2-488c-bb37-a869a93d51ce',
    entrypointTag: 'minimal_stream',
    local: true
});

await client.initialize();

// Stream the response chunk by chunk
const stream = await client.runStream({
    role: "user",
    message: "Analyze the benefits of remote work for software teams"
});

for await (const chunk of stream) {
    document.getElementById('output').innerHTML += chunk;
}
```

```rust Rust  
use runagent::client::RunAgentClient;
use serde_json::json;
use futures::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create a client with the streaming tag
    let client = RunAgentClient::new(
        "f7066c98-0eb2-488c-bb37-a869a93d51ce", 
        "minimal_stream", 
        true
    ).await?;
    
    // Stream the response chunk by chunk
    let mut stream = client.run_stream(&[
        ("role", json!("user")),
        ("message", json!("Analyze the benefits of remote work for software teams"))
    ]).await?;
    
    while let Some(chunk) = stream.next().await {
        print!("{}", chunk?);
    }
    Ok(())
}
```

```go Go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/runagent-dev/runagent/runagent-go/runagent"
)

func main() {
    // Create a client with the streaming tag
    client := runagent.NewRunAgentClient(runagent.Config{
        AgentID:       "f7066c98-0eb2-488c-bb37-a869a93d51ce",
        EntrypointTag: "minimal_stream",
        Local:         true,
    })
    
    ctx := context.Background()
    client.Initialize(ctx)
    
    // Stream the response chunk by chunk
    stream, err := client.RunStream(ctx, map[string]interface{}{
        "role":    "user",
        "message": "Analyze the benefits of remote work for software teams",
    })
    if err != nil {
        log.Fatal(err)
    }
    defer stream.Close()

    for {
        data, hasMore, err := stream.Next(ctx)
        if err != nil {
            log.Fatal(err)
        }
        if !hasMore {
            break
        }
        fmt.Printf("%v", data)
    }
}
```
</CodeGroup>

</Step>

</Steps>

**What's happening behind the scenes**: WebSocket connections, real-time data flow, and native iteration patterns are all handled automatically by RunAgent's infrastructure.

## Next Steps

- Head to the [tutorials](/tutorials/deploy-your-first-agent) guide to see the full capabilities
- Browse our [framework guides](/how-to/frameworks/langgraph) for LangGraph, CrewAI, and more
- Check out [core concepts](/explanation/core-concepts) to understand how it all works

<NeedHelp context="Introduction" />