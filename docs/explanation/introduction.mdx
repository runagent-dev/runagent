---
title: Introduction
description: 'Deploy AI agents to production in seconds, not hours'
icon: 'book-open'
---

import NeedHelp from '/snippets/need-help.mdx';

RunAgent lets you deploy Python-based AI agents and access them from any language with full streaming support.

## Key Benefits

- **Deploy agents in seconds** - Serverless infrastructure with automatic scaling
- **Access from Python, JS, Go, Rust and coming more** - Native-feeling SDKs for every language
- **Built-in streaming** - Real-time token streaming for responsive applications

## Quick Actions

<CardGroup cols={3}>
  <Card title="Deploy your first agent" icon="rocket" href="/tutorials/deploy-your-first-agent">
    Get started with a complete tutorial
  </Card>
  <Card title="Use frameworks" icon="code" href="/how-to/frameworks/langgraph">
    Integrate with popular AI frameworks
  </Card>
  <Card title="Explore SDKs" icon="terminal" href="/reference/sdk/overview">
    Access agents from any language
  </Card>
</CardGroup>

<img
  className="block dark:hidden"
  src="/images/runagent_update.svg"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/images/runagent_update.svg"
  alt="Hero Dark"
/>

## The Problem Every AI Developer Faces

Suppose you've built an incredible AI agent in Python. It uses **LangGraph for complex reasoning**, leverages **powerful tools**, and produces **amazing results**. Your team loves it!

### Then reality hits:

Your whole team is excited to use it! But the **frontend team** needs to access it in JavaScript, your **mobile app team** wants it in Kotlin, your **Unity team** wants it in C#, your **systems team** requires it in Rust.

### The traditional approach?
Build separate implementations, REST APIs, WebSocket handlers...

<Danger>**Sound exhausting?** That's because it is!</Danger>

## What RunAgent Actually Does

RunAgent fundamentally changes how AI agent deployment works. Your Python function signatures automatically become API contracts (REST or Streaming) for every supported language.

<Tip>Once `runagent.config.json` of your project points to your Python function, they are automatically converted to corresponding API endpoints, and all language SDKs automatically adapt. No API versioning, no breaking changes.</Tip>

The fastest way to experience the magic is with **RunAgent CLI**:

<Steps>

  <Step title="Install the RunAgent CLI">
  This installs the powerful RunAgent CLI, which is used to deploy and manage your agents.
   ```bash
   pip install runagent
   ```
  </Step>

  <Step title="Initialize Your Agent">

  Let's start with a minimal Agent example.
  ```bash
  runagent init my_agent
  cd my_agent
  ```
  
  This creates a new directory with everything you need to get started:
  
  ```
  my_agent/
  ├── main.py                # Your agent's functions (already defined!)
  ├── runagent.config.json   # Entrypoints already configured!
  └── requirements.txt       # Dependencies
  ```
  
  <Note>
    **What `runagent init` created for you:**
    - A `main.py` file with example agent functions (`mock_response` and `mock_response_stream`)
    - A `runagent.config.json` file with entrypoints already configured
    - All the boilerplate needed to make your Python agent accessible from any language
  </Note>
  
  The `runagent.config.json` file contains all the configuration for your agent, including:
  
  ```json
  {
    "agent_name": "my_agent",
    "description": "My AI agent",
    "framework": "default",
    "template": "default",
    "version": "1.0.0",
    "created_at": "2025-01-15T10:30:00",
    "agent_id": "f7066c98-0eb2-488c-bb37-a869a93d51ce",
    "template_source": {
      "repo_url": "https://github.com/runagent-dev/runagent.git",
      "author": "runagent-cli",
      "path": "templates/default/default"
    },
    "agent_architecture": {
      "entrypoints": [
        {
          "file": "main.py",
          "module": "mock_response",
          "tag": "minimal"
        },
        {
          "file": "main.py",
          "module": "mock_response_stream",
          "tag": "minimal_stream"
        }
      ]
    },
    "auth_settings": {
      "type": "none"
    },
    "env_vars": {}
  }
  ```
  
  <Note>
    **Key fields:**
    - `agent_id`: A unique identifier for your agent (generated automatically)
    - `agent_architecture.entrypoints`: Defines which Python functions are accessible as API endpoints
    - `auth_settings`: Authentication configuration (defaults to "none")
    - `env_vars`: Environment variables for your agent
  </Note>
  </Step>

  <Step title="Run the Agent Locally">
  
  Start your agent locally to test it:
  
  ```bash
runagent serve .
```

You will see output similar to:

```bash
Agent Details:
- Agent ID: f7066c98-0eb2-488c-bb37-a869a93d51ce
- Host: 127.0.0.1
- Port: 8451
- Framework: default
- Status: ready

Server running at: http://127.0.0.1:8451
API Documentation: http://127.0.0.1:8451/docs

Available endpoints:
- POST /api/v1/agents/.../execute/minimal - Run your agent

INFO: Uvicorn running on http://127.0.0.1:8451 (Press CTRL+C to quit)
  ```
  
  That's it! Your agent is now running and accessible through standard REST API as well as all RunAgent SDKs.
  </Step>

<Step title="Use the Agent in Your Application">

Using the RunAgent SDKs, you can access your agent from any supported language. You only need the agent ID and the entrypoint tag. Your `mock_response` function now becomes accessible in multiple languages:

<CodeGroup>

```python Python 
from runagent import RunAgentClient

# Create a client pointing to your agent
ra = RunAgentClient(
    agent_id="f7066c98-0eb2-488c-bb37-a869a93d51ce",
    entrypoint_tag="minimal",
    local=True
)

# Call your agent
agent_result = ra.run(
    role="user",
    message="Analyze the benefits of remote work for software teams"
)

print(agent_result)
```

```javascript JavaScript
import { RunAgentClient } from 'runagent';

// Create a client pointing to your agent
const ra = new RunAgentClient({
  agentId: 'f7066c98-0eb2-488c-bb37-a869a93d51ce',
  entrypointTag: 'minimal',
  local: true,
});

// Call your agent
const analysis = await ra.run({
    role: "user",
    message: "Analyze the benefits of remote work for software teams"
});

console.log(analysis);
```

```rust Rust  
use runagent::client::RunAgentClient;
use serde_json::json;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create a client pointing to your agent
    let client = RunAgentClient::new(
        "f7066c98-0eb2-488c-bb37-a869a93d51ce", 
        "minimal", 
        true
    ).await?;
    
    // Call your agent
    let response = client.run(&[
        ("role", json!("user")),
        ("message", json!("Analyze the benefits of remote work for software teams"))
    ]).await?;
    
    println!("Response: {}", response);
    Ok(())
}
```

```go Go
package main

import (
	"context"
	"fmt"
	"log"
	"time"

	"github.com/runagent-dev/runagent-go/pkg/client"
)

func main() {
	// Create a client pointing to your agent
	agentClient, err := client.NewWithAddress(
		"f7066c98-0eb2-488c-bb37-a869a93d51ce",
		"minimal",
		true,
		"localhost",
		8453,
	)
	if err != nil {
		log.Fatalf("Failed to create client: %v", err)
	}
	defer agentClient.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 2*time.Minute)
	defer cancel()

	// Call your agent
	result, err := agentClient.Run(ctx, map[string]interface{}{
		"message": "Analyze the benefits of remote work for software teams",
		"role": "user",
	})
	if err != nil {
		log.Fatalf("Failed to run agent: %v", err)
	}

	fmt.Printf("Result: %v\n", result)
}
```
</CodeGroup>

Notice how the same agent logic is accessible from all languages with idiomatic syntax for each.

</Step>
</Steps>

That's the core workflow. You're now ready to **deploy your AI agents** and **integrate them into your applications**.

## From Local Testing to Production in Seconds

Once you've tested your agent locally and confirmed everything works, deploying to production is just one command away.

**RunAgent Cloud** is the fastest AI agent deployment platform, designed to take your local agent from development to production instantly. No complex infrastructure setup, no deployment headaches.

### Deploy to Production

```bash
runagent deploy .
```

That's it. Your agent is now live and accessible globally with:

- **Automatic scaling** - Handle one request or one million
- **Global edge network** - Low latency worldwide
- **Zero infrastructure management** - Focus on your agent, not servers
- **Production-ready security** - Enterprise-grade isolation and security

Your deployed agent gets a production URL and agent ID. Simply update your SDK clients to use the production endpoint:

```python
# Switch from local to production
ra = RunAgentClient(
    agent_id="your-production-agent-id",
    entrypoint_tag="minimal",
    local=False  # Now pointing to RunAgent Cloud
)
```

The same code you tested locally now runs in a fully managed production environment.

<Card 
  title="Learn More About Deployment" 
  icon="cloud-arrow-up" 
  href="/runagent-cloud/cloud-deployment"
>
  Explore advanced deployment options, environment variables, and monitoring capabilities.
</Card>



## Agent Framework Support

RunAgent works with any Python-based AI agent framework:

<CardGroup cols={3}>
  <Card
    title="LangGraph"
    href="/how-to/frameworks/langgraph"
  >
    Deploy your LangGraph agents with built-in state management and workflow execution.
  </Card>
  <Card 
    title="CrewAI" 
    href="/how-to/frameworks/crewai"
  >
    Deploy multi-agent CrewAI systems with coordinated execution and monitoring.
  </Card>
  <Card 
    title="AutoGen" 
    href="/how-to/frameworks/autogen"
  >
    Deploy Microsoft AutoGen multi-agent conversations with step-by-step execution.
  </Card>
  <Card 
    title="AG2" 
    href="/how-to/frameworks/ag2"
  >
    Deploy AG2 conversational agents with fact-checking and multi-turn interactions.
  </Card>
  <Card 
    title="Agno" 
    href="/how-to/frameworks/agno"
  >
    Deploy Agno AI agents with print response capabilities and streaming support.
  </Card>
  <Card 
    title="Letta" 
    href="/how-to/frameworks/letta"
  >
    Deploy Letta memory-enabled agents with persistent conversations and tool integration.
  </Card>
</CardGroup>
  <Card 
    title="Custom Framework" 
    href="/how-to/frameworks/custom"
    horizontal
  >
    Use any Python-based framework by defining simple entrypoint functions.
  </Card>

## Multi-Language SDK Support

RunAgent provides native-like access to your deployed agents across multiple languages:

<CardGroup cols={2}>
  <Card
    title="Python SDK"
    icon="python"
    href="/reference/sdk/python"
  >
    Python client with streaming capabilities. Access your agents like local functions with full type safety and Python idioms.
  </Card>
  <Card 
    title="JavaScript SDK" 
    icon="js" 
    href="/reference/sdk/javascript"
  >
    Full TypeScript support with streaming and Promise-based APIs. Perfect for modern web applications with async/await patterns.
  </Card>
  <Card 
    title="Rust SDK" 
    icon="rust" 
    href="/reference/sdk/rust"
  >
    High-performance async SDK with futures and streaming support. Zero-cost abstractions for systems programming and performance-critical applications.
  </Card>
  <Card 
    title="Go SDK" 
    icon="golang" 
    href="/reference/sdk/go"
  >
    Idiomatic Go client with context-aware operations and channel-based streaming. Built for concurrent, scalable applications.
  </Card>
</CardGroup>

<Card 
  title="All Language SDKs" 
  icon="code" 
  href="/reference/sdk/overview"
  horizontal
>
  We're actively developing SDKs for additional languages including C++, C#, Java, and PHP. Want to contribute or request a specific language? Join our Discord community.
</Card>

## Real-Time Streaming Across Languages

In addition to standard REST API responses, you can also stream your agent responses seamlessly through our SDKs. 

When your entrypoint streams a response, RunAgent makes it feel native in every language SDK:

<Steps>

  <Step title="Streaming is Already Set Up">

  When you ran `runagent init`, it automatically created both a non-streaming and a streaming function for you. The streaming function is already defined in `main.py` and configured in `runagent.config.json`:

  ```python main.py
def mock_response_stream(message, role="user") -> Iterator[str]:
    """Test the mock agent with streaming responses"""
    # Your streaming agent logic here
    # Yield chunks of the response as they're generated
    for chunk in generate_response_chunks(message):
        yield chunk
  ```
  
  The `runagent.config.json` already includes this entrypoint with the tag `minimal_stream` (note the `_stream` suffix, which tells RunAgent this is a streaming endpoint).
  
  <Note>The tag for a streaming entrypoint must end with a `_stream` suffix. This is how RunAgent identifies it as a streaming endpoint.</Note>
  </Step>

  <Step title="Run the Agent Locally">
  
  Start your agent locally (the streaming endpoint is already available):
  
  ```bash
runagent serve .
```

  </Step>

<Step title="Use Streaming in Your Application">

Using the RunAgent SDKs, you can stream responses from your agent. The streaming experience feels natural in each language:

<CodeGroup>

```python Python 
from runagent import RunAgentClient

# Create a client with the streaming tag
ra = RunAgentClient(
    agent_id="f7066c98-0eb2-488c-bb37-a869a93d51ce",
    entrypoint_tag="minimal_stream",
    local=True
)

# Stream the response chunk by chunk
for chunk in ra.run(
    role="user",
    message="Analyze the benefits of remote work for software teams"
):
    print(chunk, end='', flush=True)
```

```javascript JavaScript
import { RunAgentClient } from 'runagent';

// Create a client with the streaming tag
const client = new RunAgentClient({
    agentId: 'f7066c98-0eb2-488c-bb37-a869a93d51ce',
    entrypointTag: 'minimal_stream',
    local: true
});

await client.initialize();

// Stream the response chunk by chunk
const stream = await client.runStream({
    role: "user",
    message: "Analyze the benefits of remote work for software teams"
});

for await (const chunk of stream) {
    document.getElementById('output').innerHTML += chunk;
}
```

```rust Rust  
use runagent::client::RunAgentClient;
use serde_json::json;
use futures::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create a client with the streaming tag
    let client = RunAgentClient::new(
        "f7066c98-0eb2-488c-bb37-a869a93d51ce", 
        "minimal_stream", 
        true
    ).await?;
    
    // Stream the response chunk by chunk
    let mut stream = client.run_stream(&[
        ("role", json!("user")),
        ("message", json!("Analyze the benefits of remote work for software teams"))
    ]).await?;
    
    while let Some(chunk) = stream.next().await {
        print!("{}", chunk?);
    }
    Ok(())
}
```

```go Go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/runagent-dev/runagent/runagent-go/runagent"
)

func main() {
    // Create a client with the streaming tag
    client := runagent.NewRunAgentClient(runagent.Config{
        AgentID:       "f7066c98-0eb2-488c-bb37-a869a93d51ce",
        EntrypointTag: "minimal_stream",
        Local:         true,
    })
    
    ctx := context.Background()
    client.Initialize(ctx)
    
    // Stream the response chunk by chunk
    stream, err := client.RunStream(ctx, map[string]interface{}{
        "role":    "user",
        "message": "Analyze the benefits of remote work for software teams",
    })
    if err != nil {
        log.Fatal(err)
    }
    defer stream.Close()

    for {
        data, hasMore, err := stream.Next(ctx)
        if err != nil {
            log.Fatal(err)
        }
        if !hasMore {
            break
        }
        fmt.Printf("%v", data)
    }
}
```
</CodeGroup>

</Step>

</Steps>

**What's happening behind the scenes**: WebSocket connections, real-time data flow, and native iteration patterns are all handled automatically by RunAgent's infrastructure.

## Next Steps

- Head to the [tutorials](/tutorials/deploy-your-first-agent) guide to see the full capabilities
- Browse our [framework guides](/how-to/frameworks/langgraph) for LangGraph, CrewAI, and more
- Check out [core concepts](/explanation/core-concepts) to understand how it all works

<NeedHelp context="Introduction" />